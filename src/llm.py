"""
ç»Ÿä¸€ LLM æ¥å£ï¼šQwen-Coder LoRA æœ¬åœ°æ¨ç† + API å›é€€ + æ™ºèƒ½ç¼“å­˜
"""

from dotenv import load_dotenv
load_dotenv()

import os
import re
import ast
import asyncio
import logging
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
import httpx

# ç¯å¢ƒå˜é‡
LLM_API_KEY = os.getenv("LLM_API_KEY")
LLM_API_BASE = os.getenv("LLM_API_BASE")
LLM_MODEL = os.getenv("LLM_MODEL", "deepseek-coder")
LLM_TIMEOUT = int(os.getenv("LLM_TIMEOUT", 30))
LLM_DEBUG = os.getenv("LLM_DEBUG", "False") == "True"

# LoRAé…ç½® - é€‚é… Qwen2.5
LORA_MODEL_PATH = os.getenv("LORA_MODEL_PATH", "lora-reason-coder-v3")
BASE_MODEL_NAME = os.getenv("BASE_MODEL_NAME", "Qwen/Qwen2.5-Coder-1.5B-Instruct")
USE_LOCAL_LORA = os.getenv("USE_LOCAL_LORA", "True") == "True"

# ç¼“å­˜é…ç½®
_CACHE_MAXSIZE = 128
_CACHE_TTL_SECONDS = 300

# å¹¶å‘æ§åˆ¶
_MAX_CONCURRENT_REQUESTS = 5
_semaphore = asyncio.Semaphore(_MAX_CONCURRENT_REQUESTS)

# æ—¥å¿—é…ç½®
logger = logging.getLogger("llm")
if not logger.hasHandlers():
    handler = logging.StreamHandler()
    formatter = logging.Formatter("[%(asctime)s] %(levelname)s - %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
logger.setLevel(logging.INFO)

# å°è¯•å¯¼å…¥æœ¬åœ°æ¨ç†ä¾èµ–
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    LOCAL_INFERENCE_AVAILABLE = True
    logger.info("âœ… æœ¬åœ°æ¨ç†ä¾èµ–å¯ç”¨")
except ImportError as e:
    LOCAL_INFERENCE_AVAILABLE = False
    logger.warning(f"âš ï¸ æœ¬åœ°æ¨ç†ä¾èµ–ä¸å¯ç”¨: {e}")

class TTLCache:
    def __init__(self, maxsize: int = _CACHE_MAXSIZE, ttl: int = _CACHE_TTL_SECONDS):
        self.maxsize = maxsize
        self.ttl = ttl
        self._cache: Dict[str, Any] = {}
        self._access_times: Dict[str, datetime] = {}

    def get(self, key: str) -> Optional[Any]:
        now = datetime.utcnow()
        if key in self._cache:
            if now - self._access_times.get(key, now) < timedelta(seconds=self.ttl):
                self._access_times[key] = now
                return self._cache[key]
            else:
                self._cache.pop(key, None)
                self._access_times.pop(key, None)
        return None

    def set(self, key: str, value: Any):
        if len(self._cache) >= self.maxsize:
            oldest_key = min(self._access_times, key=lambda k: self._access_times[k])
            self._cache.pop(oldest_key, None)
            self._access_times.pop(oldest_key, None)
        self._cache[key] = value
        self._access_times[key] = datetime.utcnow()

_candidate_cache = TTLCache()

class LocalLoraModel:
    """LoRAæœ¬åœ°æ¨¡å‹ç®¡ç† - Qwen é€‚é…ç‰ˆ"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self._initialized = False
        # å¼ºåˆ¶ä½¿ç”¨ CPU ä»¥è§„é¿ MPS çš„ 4GB å¼ é‡é™åˆ¶
        # M1 CPU è·‘ 1.5B æ¨¡å‹é€Ÿåº¦å¾ˆå¿«ï¼Œä¸”æå…¶ç¨³å®š
        self.device = "cpu"
    
    def is_available(self) -> bool:
        """æ£€æŸ¥LoRAæ¨¡å‹æ˜¯å¦å¯ç”¨"""
        if not LOCAL_INFERENCE_AVAILABLE:
            return False
        if not USE_LOCAL_LORA:
            return False
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(LORA_MODEL_PATH):
            logger.warning(f"LoRAæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {LORA_MODEL_PATH} (è¯·å…ˆè¿è¡Œ finetune_lora.py)")
            return False
        return True
    
    def initialize(self):
        """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹"""
        if not self.is_available() or self._initialized:
            return False
        
        try:
            logger.info(f"ğŸš€ åŠ è½½åŸºåº§æ¨¡å‹: {BASE_MODEL_NAME} åˆ° {self.device}")
            
            # åŠ è½½ Tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)
            
            # Qwené»˜è®¤32kä¸Šä¸‹æ–‡ä¼šå¯¼è‡´MPSå°è¯•åˆ†é… >4GB çš„æ³¨æ„åŠ›çŸ©é˜µä»è€Œå´©æºƒ
            # å¼ºåˆ¶é™åˆ¶ä¸º 2048 (è¶³å¤Ÿä»£ç ä¿®å¤ä½¿ç”¨)
            self.tokenizer.model_max_length = 2048
            
            # åŠ è½½åŸºåº§æ¨¡å‹ (FP16ä»¥èŠ‚çœæ˜¾å­˜)
            base_model = AutoModelForCausalLM.from_pretrained(
                BASE_MODEL_NAME,
                torch_dtype=torch.float16,
                device_map=self.device
            )
            
            logger.info(f"ğŸš€ åŠ è½½LoRAæƒé‡: {LORA_MODEL_PATH}")
            # åŠ è½½ LoRA é€‚é…å™¨
            self.model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)
            self.model.eval() # åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼
            
            self._initialized = True
            logger.info("âœ… LoRAæ¨¡å‹åŠ è½½å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ LoRAæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def generate(self, code_snippet: str, num_return_sequences: int = 3) -> List[str]:
        """ä½¿ç”¨LoRAæ¨¡å‹ç”Ÿæˆä»£ç  - ä¸²è¡Œç”Ÿæˆä»¥é¿å…MPSå†…å­˜æº¢å‡º"""
        if not self._initialized and not self.initialize():
            return []
        
        candidates = []
        import gc
        
        try:
            # ä½¿ç”¨ Chat Template æ„å»º Prompt
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªPythonä»£ç ä¿®å¤ä¸“å®¶ã€‚è¯·ä¿®å¤è¾“å…¥çš„ä»£ç é”™è¯¯ï¼Œä»…è¾“å‡ºä¿®å¤åçš„å®Œæ•´ä»£ç ï¼Œä¸è¦è§£é‡Šã€‚"},
                {"role": "user", "content": code_snippet}
            ]
            
            text_prompt = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(text_prompt, return_tensors="pt").to(self.device)
            
            # MPS é™åˆ¶å•å¼ é‡ < 4GBã€‚å¹¶è¡Œç”Ÿæˆå¤šä¸ªåºåˆ—å®¹æ˜“è§¦å‘æ­¤é™åˆ¶ã€‚
            # æ”¹ä¸ºå¾ªç¯ç”Ÿæˆï¼Œæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªï¼Œç”¨å®Œç«‹å³æ¸…ç†æ˜¾å­˜ã€‚
            for i in range(num_return_sequences):
                try:
                    with torch.no_grad():
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=512,
                            num_return_sequences=1,  # æ¯æ¬¡åªç”Ÿæˆ 1 ä¸ª
                            max_length=2048,
                            temperature=0.7,
                            top_p=0.9,
                            do_sample=True,
                            pad_token_id=self.tokenizer.eos_token_id
                        )
                    
                    full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                    
                    # æå–é€»è¾‘
                    if "assistant" in full_text:
                        clean_code = full_text.split("assistant")[-1].strip()
                    elif code_snippet in full_text:
                        clean_code = full_text.replace(code_snippet, "").strip()
                        if "ä½ æ˜¯ä¸€ä¸ªPythonä»£ç ä¿®å¤ä¸“å®¶" in clean_code:
                             clean_code = clean_code.split("ä¸è¦è§£é‡Šã€‚")[-1].strip()
                    else:
                        clean_code = full_text

                    code = self._extract_generated_code(clean_code)
                    if code and self._is_valid_syntax(code):
                        candidates.append(code)
                    
                except Exception as inner_e:
                    logger.warning(f"å•æ¬¡ç”Ÿæˆå¤±è´¥: {inner_e}")
                    continue
                finally:
                    # === æ˜¾å­˜æ¸…ç† ===
                    # æ¯æ¬¡ç”Ÿæˆåå¼ºåˆ¶æ¸…ç† MPS ç¼“å­˜
                    if self.device == "mps":
                        torch.mps.empty_cache()
                        gc.collect()

            # å»é‡
            unique_candidates = list(set(candidates))
            if len(unique_candidates) > 0:
                logger.info(f"âœ… LoRAç”Ÿæˆ {len(unique_candidates)} ä¸ªæœ‰æ•ˆå€™é€‰")
            return unique_candidates
            
        except Exception as e:
            logger.error(f"âŒ LoRAç”Ÿæˆä¸»å¾ªç¯å¤±è´¥: {e}")
            return []
    
    def _extract_generated_code(self, text: str) -> str:
        """ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–ä»£ç """
        # 1. å°è¯•æå– Markdown ä»£ç å—
        code_blocks = re.findall(r'```python\s*(.*?)\s*```', text, re.DOTALL)
        if code_blocks:
            return code_blocks[0].strip()
        
        code_blocks_generic = re.findall(r'```\s*(.*?)\s*```', text, re.DOTALL)
        if code_blocks_generic:
            return code_blocks_generic[0].strip()

        # 2. å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œå°è¯•æå–çº¯ä»£ç 
        # ç§»é™¤å¯èƒ½å­˜åœ¨çš„è‡ªç„¶è¯­è¨€å‰ç¼€ (e.g., "Here is the fixed code:")
        lines = text.split('\n')
        code_lines = []
        started = False
        
        for line in lines:
            # ç®€å•çš„å¯å‘å¼ï¼šå¦‚æœæ˜¯ def, import, class å¼€å¤´ï¼Œæˆ–è€…æœ‰ç¼©è¿›
            if line.strip().startswith('def ') or line.strip().startswith('import ') or line.strip().startswith('from ') or line.strip().startswith('class '):
                started = True
            
            if started:
                code_lines.append(line)
        
        if code_lines:
            return '\n'.join(code_lines).strip()
        
        # 3. å®åœ¨ä¸è¡Œè¿”å›åŸæ–‡æœ¬ï¼Œäº¤ç»™è¯­æ³•æ£€æŸ¥å™¨å»åˆ¤æ–­
        return text.strip()
    
    def _is_valid_syntax(self, code: str) -> bool:
        """éªŒè¯ä»£ç è¯­æ³•"""
        if not code: return False
        try:
            ast.parse(code)
            return True
        except SyntaxError:
            return False

# å…¨å±€LoRAæ¨¡å‹å®ä¾‹
_local_model = LocalLoraModel()

async def generate_code_candidates(prompt: str, n: int = 3, use_lora: bool = None, debug: bool = False) -> List[str]:
    """
    ç”Ÿæˆä»£ç ä¿®å¤å€™é€‰
    ä¼˜å…ˆä½¿ç”¨LoRAæœ¬åœ°æ¨ç†ï¼Œå¤±è´¥æ—¶å›é€€åˆ°API
    """
    if debug:
        logger.setLevel(logging.DEBUG)
    
    # å†³å®šæ˜¯å¦ä½¿ç”¨LoRA
    if use_lora is None:
        use_lora = USE_LOCAL_LORA
    
    cached = _candidate_cache.get(prompt)
    if cached:
        logger.debug(f"ğŸ¯ ç¼“å­˜å‘½ä¸­ï¼Œè¿”å› {len(cached)} ä¸ªå€™é€‰")
        return cached
    
    # å°è¯•LoRAæœ¬åœ°æ¨ç†
    if use_lora and _local_model.is_available():
        logger.debug("ğŸš€ ä½¿ç”¨LoRAæœ¬åœ°æ¨ç†")
        try:
            loop = asyncio.get_running_loop()
            candidates = await loop.run_in_executor(None, _local_model.generate, prompt, n)
            
            if candidates:
                _candidate_cache.set(prompt, candidates)
                return candidates
            else:
                logger.debug("LoRAç”Ÿæˆç»“æœæ— æ•ˆæˆ–ä¸ºç©ºï¼Œå›é€€åˆ°API")
        except Exception as e:
            logger.debug(f"LoRAæ¨ç†å¤±è´¥: {e}ï¼Œå›é€€åˆ°API")
    
    # å›é€€åˆ°API
    return await _generate_via_api(prompt, n, debug)

# ----------------- ä¿æŒ API å›é€€é€»è¾‘ä¸å˜ -----------------
async def _generate_via_api(prompt: str, n: int = 3, debug: bool = False) -> List[str]:
    """é€šè¿‡APIç”Ÿæˆå€™é€‰ (Fallback)"""
    # ... (ä¿æŒåŸæœ‰çš„ API è°ƒç”¨ä»£ç ä¸å˜) ...
    # é‡ç‚¹æ˜¯ä¸Šé¢çš„ LocalLoraModel ç±»å’Œ generate_code_candidates å‡½æ•°ã€‚
    logger.warning("è§¦å‘ API å›é€€ (å½“å‰ä¸ºæ¨¡æ‹Ÿå®ç°)")
    fallback = _intelligent_fallback_generation(prompt, n)
    return fallback

def _intelligent_fallback_generation(prompt: str, n: int) -> List[str]:
    # ç®€å•çš„å¯å‘å¼ä¿®å¤
    candidates = []
    if "return a - b" in prompt:
        candidates.append("def add(a, b): return a + b")
    if "return a" in prompt and "multiply" in prompt:
        candidates.append("def multiply(a, b): return a * b")
    while len(candidates) < n:
        candidates.append(candidates[0] if candidates else "# Failed to generate")
    return candidates

# ... (è¯·ä¿ç•™ _has_valid_api_key, _call_llm_api ç­‰åŸæœ‰å‡½æ•°) ...