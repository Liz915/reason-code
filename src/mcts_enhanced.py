import math
import asyncio
from typing import Optional, List, Any, Dict
from dataclasses import dataclass, field


from llm import generate_code_candidates
from sandbox import execute_code
from evaluator import evaluate_code
from config import MCTS_C
from retriever import simple_retrieve 

@dataclass
class Node:
    code: str
    parent: Optional["Node"]
    visits: int = 0
    wins: float = 0.0
    children: List["Node"] = field(default_factory=list)
    last_result: Any = None
    evaluation_result: Any = None  # æ–°å¢ï¼šè¯„ä¼°ç»“æœ

    def ucb_score(self, c: float = MCTS_C):
        if self.visits == 0:
            return float("inf")
        parent_visits = self.parent.visits if self.parent else 1
        return (self.wins / self.visits) + c * math.sqrt(math.log(parent_visits) / self.visits)

class EnhancedMCTS:
    """å¢å¼ºç‰ˆMCTSï¼šé›†æˆåˆ†çº§è¯„ä¼°"""
    
    def __init__(self, root_code: str, n_simulations: int = 30, n_candidates: int = 3):
        self.root = Node(code=root_code, parent=None)
        self.n_simulations = n_simulations
        self.n_candidates = n_candidates
        self.stats = {
            "syntax_checks": 0,
            "static_analyses": 0, 
            "runtime_tests": 0,
            "early_rejects": 0
        }

    async def run(self, test_runner: str):
        print(f"ğŸ” å¼€å§‹MCTSæœç´¢ï¼Œæ¨¡æ‹Ÿæ¬¡æ•°: {self.n_simulations}")
        print("âš¡ ä¼˜åŒ–æ¨¡å¼: åˆ†çº§è¯„ä¼°å¾ªç¯")
        
        for i in range(self.n_simulations):
            node = self._select(self.root)
            reward = await self._expand_and_simulate(node, test_runner)
            self._backpropagate(node, reward)
            
            if (i + 1) % 5 == 0:  # ç¨å¾®é¢‘ç¹ä¸€ç‚¹æ‰“å°è¿›åº¦
                self._print_progress(i + 1)
        
        best = self._get_best_child()
        final_code = best.code if best else self.root.code
        
        self._print_final_stats()
        return final_code

    def _select(self, node: Node) -> Node:
        while node.children:
            node = max(node.children, key=lambda n: n.ucb_score())
        return node

    def _get_best_child(self):
        return max(self.root.children, key=lambda n: (n.wins / n.visits) if n.visits > 0 else -1, default=None)

    async def _expand_and_simulate(self, node: Node, test_runner: str) -> float:
        prompt = self._build_prompt(node, test_runner)
        # å¹¶å‘è¯·æ±‚ LLM ç”Ÿæˆå€™é€‰
        # æ³¨æ„ï¼šllm.py å†…éƒ¨å·²ç»åšäº†ä¸²è¡ŒåŒ–å¤„ç†ä»¥é€‚åº” MPSï¼Œè¿™é‡Œæ— éœ€æ”¹åŠ¨æ¥å£
        candidates = await generate_code_candidates(prompt, n=self.n_candidates)

        # å¹¶å‘è¯„ä¼°æ‰€æœ‰å€™é€‰
        from evaluator import evaluate_candidates_async
        eval_results = await evaluate_candidates_async(candidates, test_runner, prompt)

        best_reward = 0.0

        from self_correction import attempt_fix

        for cand, eval_result in zip(candidates, eval_results):
            
            final_code = cand
            final_result = eval_result
            
            # ğŸš‘ æŠ¢æ•‘æœºåˆ¶ï¼šå¦‚æœè¿è¡Œæ—¶å¤±è´¥ (å¾—åˆ†0.7)ï¼Œå°è¯•ä¿®å¤
            if eval_result["overall"]["reward"] == 0.7:
                failed_level = eval_result["overall"]["failed_at"]
                if failed_level == "level_3": # è¿è¡Œæ—¶é”™è¯¯
                    error_msg = eval_result[failed_level]["message"]
                    
                    # å°è¯•ä¿®å¤
                    fixed_code = await attempt_fix(cand, error_msg, test_runner)
                    
                    if fixed_code != cand:
                        # é‡æ–°è¯„ä¼°ä¿®å¤åçš„ä»£ç 
                        # è¿™é‡Œç®€å•åŒæ­¥è°ƒç”¨ evaluateï¼Œæˆ–è€…ä½ å¯ä»¥å°è£…æˆ await
                        from evaluator import evaluate_code
                        new_result = evaluate_code(fixed_code, test_runner)
                        
                        if new_result["overall"]["reward"] > 0.7:
                            print(f"âœ¨ ä¿®å¤æˆåŠŸ! å¾—åˆ†æå‡: 0.7 -> {new_result['overall']['reward']}")
                            final_code = fixed_code
                            final_result = new_result
                        else:
                            print("   ä¿®å¤æœªç”Ÿæ•ˆ")

            child = Node(code=final_code, parent=node)
            node.children.append(child)

            child.evaluation_result = final_result
            child.last_result = final_result.get("overall", {})
            node.children.append(child)

            child.evaluation_result = eval_result
            child.last_result = eval_result.get("overall", {})

            # æ›´æ–°ç»Ÿè®¡
            self._update_stats(eval_result)

            reward = eval_result.get("overall", {}).get("reward", 0.0)
            child.visits += 1
            child.wins += reward

            if reward > best_reward:
                best_reward = reward
                if reward == 1.0:
                    print("  âœ… æ‰¾åˆ°å®Œå…¨é€šè¿‡çš„å€™é€‰")
        return best_reward

    def _update_stats(self, eval_result: dict):
        """æ›´æ–°åˆ†çº§è¯„ä¼°ç»Ÿè®¡"""
        for level in ["level_1", "level_2", "level_3"]:
            if level in eval_result:
                if level == "level_1":
                    self.stats["syntax_checks"] += 1
                elif level == "level_2":
                    self.stats["static_analyses"] += 1
                elif level == "level_3":
                    self.stats["runtime_tests"] += 1
                
                if not eval_result[level]["passed"] and level != "level_3":
                    self.stats["early_rejects"] += 1

    def _build_prompt(self, node: Node, test_runner: str) -> str:
        prompt = f"å½“å‰ä»£ç :\n```python\n{node.code}\n```\n\n"
        
        # ç®€å•çš„ RAG æ£€ç´¢
        retrieved = simple_retrieve(node.code, k=3)
        if retrieved:
            prompt += "\n\n# ä»¥ä¸‹æ˜¯è¿‡å»ç±»ä¼¼å¤±è´¥çš„ä¿®å¤å‚è€ƒï¼š"
            for r in retrieved:
                prompt += f"\n# å¤±è´¥å€™é€‰: {r['candidate']}"
                prompt += f"\n# é”™è¯¯: {r['stderr']}"

        if node.evaluation_result:
            failed_level = node.evaluation_result["overall"]["failed_at"]
            if failed_level:
                level_msg = node.evaluation_result[failed_level]["message"]
                prompt += f"åœ¨{failed_level}å¤±è´¥: {level_msg}\n\n"
        
        prompt += "è¯·ä¿®å¤ä»£ç ä½¿å…¶é€šè¿‡æµ‹è¯•ã€‚åªè¿”å›ä¿®å¤åçš„Pythonä»£ç ã€‚"
        return prompt

    def _backpropagate(self, node: Node, reward: float):
        cur = node
        while cur:
            cur.visits += 1
            cur.wins += reward
            cur = cur.parent

    def _print_progress(self, current_iter: int):
        best_child = self._get_best_child()
        best_rate = (best_child.wins / best_child.visits) if best_child and best_child.visits > 0 else 0
        total_nodes = len(self.root.children) if self.root.children else 0
        
        print(f"   è¿›åº¦: {current_iter}/{self.n_simulations}, "
              f"æœ€ä½³é€šè¿‡ç‡: {best_rate:.2f}, æ ¹èŠ‚ç‚¹åˆ†æ”¯: {total_nodes}")

    def _print_final_stats(self):
        print(f"ğŸ“Š åˆ†çº§è¯„ä¼°ç»Ÿè®¡:")
        print(f"   è¯­æ³•æ£€æŸ¥: {self.stats['syntax_checks']}æ¬¡")
        print(f"   é™æ€åˆ†æ: {self.stats['static_analyses']}æ¬¡") 
        print(f"   è¿è¡Œæ—¶æµ‹è¯•: {self.stats['runtime_tests']}æ¬¡")
        print(f"   æ—©æœŸæ‹’ç»: {self.stats['early_rejects']}æ¬¡")
        
        if self.stats['syntax_checks'] > 0:
            reject_rate = self.stats['early_rejects'] / self.stats['syntax_checks']
            print(f"   æ—©æœŸæ‹’ç»ç‡: {reject_rate:.1%}")