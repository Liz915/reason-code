"""
ä½¿ç”¨ HumanEval æ•°æ®é›†æµ‹è¯• Agent èƒ½åŠ›
"""
import sys
import os
import asyncio
import json
import textwrap  # ğŸ‘ˆ æ–°å¢å¼•å…¥

# ç¡®ä¿è·¯å¾„æ­£ç¡®
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from mcts_enhanced import EnhancedMCTS
from evaluator import evaluate_code

# HumanEval ç¬¬ 0 é¢˜
HUMAN_EVAL_0 = {
    "task_id": "HumanEval/0",
    "prompt": "from typing import List\n\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0], 0.3)\n    True\n    \"\"\"\n",
    "test": "\n\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\n\n\ndef check(candidate):\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n\n"
}

async def solve_humaneval():
    print(f"ğŸ§  æŒ‘æˆ˜ HumanEval/0: has_close_elements")
    print("-" * 50)
    
    problem_prompt = HUMAN_EVAL_0["prompt"]
    
    # åŸå§‹æµ‹è¯•é€»è¾‘
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å»æ‰äº†å¤–å±‚çš„ if __name__ == '__main__':
    # å› ä¸º sandbox.py ä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬è¦åŠ è¿™ä¸ªå¤´
    raw_test_runner = f"""
{HUMAN_EVAL_0['test']}

# ç›´æ¥è°ƒç”¨ checkï¼Œå› ä¸ºå¤–å±‚å·²ç»è¢« sandbox åŒ…è£¹åœ¨ main é‡Œäº†
# è¿™é‡Œçš„ has_close_elements æ¥è‡ª submission.py (åœ¨ sandbox ç¯å¢ƒä¸­)
# ä½†ç”±äº sandbox ç›´æ¥æ‹¼æ¥æ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿å­˜å…¥ submission çš„ä»£ç èƒ½è¢«è®¿é—®
# å®é™…ä¸Š sandbox æŠŠä»£ç å’Œæµ‹è¯•æ‹¼åœ¨åŒä¸€ä¸ªæ–‡ä»¶é‡Œï¼Œæ‰€ä»¥ç›´æ¥è°ƒç”¨å³å¯

try:
    check(has_close_elements)
    print("ALL TESTS PASSED")
except AssertionError:
    print("TEST FAILED")
    exit(1)
except Exception as e:
    print(f"ERROR: {{e}}")
    exit(1)
"""

    
    
    test_runner = textwrap.indent(raw_test_runner, '    ')

    mcts = EnhancedMCTS(
        root_code=problem_prompt, 
        n_simulations=10,
        n_candidates=1
    )
    
    best_code = await mcts.run(test_runner)
    
    print("\nğŸ‰ æœ€ç»ˆç”Ÿæˆçš„ä»£ç :")
    print("=" * 40)
    print(best_code)
    print("=" * 40)

if __name__ == "__main__":
    asyncio.run(solve_humaneval())